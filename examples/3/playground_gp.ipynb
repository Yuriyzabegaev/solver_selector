{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firedrake/workspace/porepy_workspace/porepy/src/porepy/numerics/nonlinear/nonlinear_solvers.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from data_scripts import get_newest_data_paths\n",
    "from mandel_model import make_mandel_setup\n",
    "from mandel_solvers import make_mandel_solver_space\n",
    "\n",
    "from solver_selector.simulation_runner import make_simulation_runner\n",
    "\n",
    "path = Path().parent / \"../2\"\n",
    "load_data_paths = []\n",
    "# load_data_paths += get_newest_data_paths(path / \"poro_coldstart_s\", n_newest=1)\n",
    "# load_data_paths += get_newest_data_paths(path / \"poro_coldstart_m\", n_newest=1)\n",
    "load_data_paths += get_newest_data_paths('poro_coldstart_gp', n_newest=1)\n",
    "\n",
    "# assert len(load_data_paths) == 3\n",
    "\n",
    "solver_space = make_mandel_solver_space(l_factor=\"dynamic\")\n",
    "simulation = make_mandel_setup(model_size=\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting from 1 solvers.\n",
      "0 gmres - splitting_fixed_stress [primary - direct, secondary - direct, l_factor=0, primary_variable, method]\n",
      "Using Gaussian process, exploration: 0.1\n",
      "Warm start using data:\n",
      "/home/firedrake/workspace/porepy_workspace/solver_selector/examples/3/performance/poro_coldstart_gp_13.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firedrake/firedrake/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 2 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simulation_runner = make_simulation_runner(\n",
    "    solver_space=solver_space,\n",
    "    params={\n",
    "        \"load_statistics_paths\": load_data_paths,\n",
    "        \"print_solver\": True,\n",
    "        \"predictor\": \"gaussian_process\",\n",
    "        # 'regressor': 'mlp',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>ms</th>\n",
       "      <th>cfl_mean</th>\n",
       "      <th>cfl_max</th>\n",
       "      <th>l_factor</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>4.767208</td>\n",
       "      <td>2.674774</td>\n",
       "      <td>0.516842</td>\n",
       "      <td>-0.939904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.570612e-15</td>\n",
       "      <td>0.714227</td>\n",
       "      <td>0.073844</td>\n",
       "      <td>0.113791</td>\n",
       "      <td>0.053294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>4.250704</td>\n",
       "      <td>2.651331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.174203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>4.380022</td>\n",
       "      <td>2.657281</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>-0.930906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>4.566636</td>\n",
       "      <td>2.667424</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>-0.925757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>4.896689</td>\n",
       "      <td>2.676695</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>-0.921026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.023013e+01</td>\n",
       "      <td>9.988645</td>\n",
       "      <td>3.395433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.908952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ts            ms    cfl_mean     cfl_max    l_factor  \\\n",
       "count  100.000000  1.000000e+02  100.000000  100.000000  100.000000   \n",
       "mean     2.302585  1.023013e+01    4.767208    2.674774    0.516842   \n",
       "std      0.000000  3.570612e-15    0.714227    0.073844    0.113791   \n",
       "min      2.302585  1.023013e+01    4.250704    2.651331    0.000000   \n",
       "25%      2.302585  1.023013e+01    4.380022    2.657281    0.526316   \n",
       "50%      2.302585  1.023013e+01    4.566636    2.667424    0.526316   \n",
       "75%      2.302585  1.023013e+01    4.896689    2.676695    0.526316   \n",
       "max      2.302585  1.023013e+01    9.988645    3.395433    1.000000   \n",
       "\n",
       "           target  \n",
       "count  100.000000  \n",
       "mean    -0.939904  \n",
       "std      0.053294  \n",
       "min     -1.174203  \n",
       "25%     -0.930906  \n",
       "50%     -0.925757  \n",
       "75%     -0.921026  \n",
       "max     -0.908952  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "predictor = simulation_runner.solver_selector.predictors[0]\n",
    "X = np.array(predictor.memory_contexts)\n",
    "y = np.array(predictor.memory_rewards)\n",
    "\n",
    "data = pd.DataFrame(X, columns=['ts', 'ms', 'cfl_mean', 'cfl_max', 'l_factor'])\n",
    "data['target'] = y\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mean = data.ts.mean()\n",
    "# data_max = data.ms.max() + (data.ms.max() - data.ms.min())\n",
    "data_max = data.ms.max()\n",
    "ms = np.linspace(data.ms.min(), data_max, 100)\n",
    "cfl_mean = data.cfl_mean.mean()\n",
    "cfl_max = data.cfl_max.mean()\n",
    "l_factor = np.linspace(data.l_factor.min(), data.l_factor.max(), 100)\n",
    "\n",
    "ms, l_factor = np.meshgrid(ms, l_factor, indexing='ij')\n",
    "\n",
    "data1 = np.zeros((5, 100, 100))\n",
    "data1[0] = ts_mean\n",
    "data1[1] = ms\n",
    "data1[2] = cfl_mean\n",
    "data1[3] = cfl_max\n",
    "data1[4] = l_factor\n",
    "data1 = data1.reshape(5, -1)\n",
    "data1 = data1.T\n",
    "\n",
    "target1 = predictor.regressor.predict(data1)\n",
    "df1 = pd.DataFrame(data1, columns=['ts', 'ms', 'cfl_mean', 'cfl_max', 'l_factor'])\n",
    "df1['target'] = target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = predictor.regressor.predict(data1, return_std=True)\n",
    "# df1['target'] = mean + std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ffa602ca5f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dash_app import make_app\n",
    "\n",
    "app = make_app([data, df1], ['ms', 'l_factor', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=5)\n",
    "            + gpytorch.kernels.LinearKernel(active_dims=[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# initialize likelihood and model\n",
    "predictor = simulation_runner.solver_selector.predictors[0]\n",
    "X = torch.Tensor(predictor.memory_contexts)\n",
    "y = torch.Tensor(predictor.memory_rewards)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X, y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firedrake/firedrake/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:\n",
      "\n",
      "A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 0.821   noise: 0.693\n",
      "Iter 2/100 - Loss: 0.782   noise: 0.644\n",
      "Iter 3/100 - Loss: 0.743   noise: 0.598\n",
      "Iter 4/100 - Loss: 0.702   noise: 0.554\n",
      "Iter 5/100 - Loss: 0.662   noise: 0.513\n",
      "Iter 6/100 - Loss: 0.620   noise: 0.474\n",
      "Iter 7/100 - Loss: 0.578   noise: 0.437\n",
      "Iter 8/100 - Loss: 0.535   noise: 0.403\n",
      "Iter 9/100 - Loss: 0.492   noise: 0.370\n",
      "Iter 10/100 - Loss: 0.448   noise: 0.340\n",
      "Iter 11/100 - Loss: 0.404   noise: 0.312\n",
      "Iter 12/100 - Loss: 0.359   noise: 0.286\n",
      "Iter 13/100 - Loss: 0.313   noise: 0.261\n",
      "Iter 14/100 - Loss: 0.267   noise: 0.239\n",
      "Iter 15/100 - Loss: 0.221   noise: 0.218\n",
      "Iter 16/100 - Loss: 0.174   noise: 0.198\n",
      "Iter 17/100 - Loss: 0.126   noise: 0.181\n",
      "Iter 18/100 - Loss: 0.078   noise: 0.164\n",
      "Iter 19/100 - Loss: 0.030   noise: 0.149\n",
      "Iter 20/100 - Loss: -0.019   noise: 0.136\n",
      "Iter 21/100 - Loss: -0.067   noise: 0.123\n",
      "Iter 22/100 - Loss: -0.116   noise: 0.111\n",
      "Iter 23/100 - Loss: -0.166   noise: 0.101\n",
      "Iter 24/100 - Loss: -0.215   noise: 0.091\n",
      "Iter 25/100 - Loss: -0.265   noise: 0.083\n",
      "Iter 26/100 - Loss: -0.315   noise: 0.075\n",
      "Iter 27/100 - Loss: -0.364   noise: 0.068\n",
      "Iter 28/100 - Loss: -0.414   noise: 0.061\n",
      "Iter 29/100 - Loss: -0.464   noise: 0.055\n",
      "Iter 30/100 - Loss: -0.514   noise: 0.050\n",
      "Iter 31/100 - Loss: -0.563   noise: 0.045\n",
      "Iter 32/100 - Loss: -0.613   noise: 0.041\n",
      "Iter 33/100 - Loss: -0.662   noise: 0.037\n",
      "Iter 34/100 - Loss: -0.711   noise: 0.033\n",
      "Iter 35/100 - Loss: -0.760   noise: 0.030\n",
      "Iter 36/100 - Loss: -0.808   noise: 0.027\n",
      "Iter 37/100 - Loss: -0.855   noise: 0.024\n",
      "Iter 38/100 - Loss: -0.903   noise: 0.022\n",
      "Iter 39/100 - Loss: -0.949   noise: 0.020\n",
      "Iter 40/100 - Loss: -0.995   noise: 0.018\n",
      "Iter 41/100 - Loss: -1.041   noise: 0.016\n",
      "Iter 42/100 - Loss: -1.085   noise: 0.015\n",
      "Iter 43/100 - Loss: -1.128   noise: 0.013\n",
      "Iter 44/100 - Loss: -1.171   noise: 0.012\n",
      "Iter 45/100 - Loss: -1.212   noise: 0.011\n",
      "Iter 46/100 - Loss: -1.253   noise: 0.010\n",
      "Iter 47/100 - Loss: -1.291   noise: 0.009\n",
      "Iter 48/100 - Loss: -1.329   noise: 0.008\n",
      "Iter 49/100 - Loss: -1.365   noise: 0.007\n",
      "Iter 50/100 - Loss: -1.400   noise: 0.007\n",
      "Iter 51/100 - Loss: -1.433   noise: 0.006\n",
      "Iter 52/100 - Loss: -1.465   noise: 0.005\n",
      "Iter 53/100 - Loss: -1.495   noise: 0.005\n",
      "Iter 54/100 - Loss: -1.524   noise: 0.005\n",
      "Iter 55/100 - Loss: -1.551   noise: 0.004\n",
      "Iter 56/100 - Loss: -1.578   noise: 0.004\n",
      "Iter 57/100 - Loss: -1.603   noise: 0.004\n",
      "Iter 58/100 - Loss: -1.628   noise: 0.003\n",
      "Iter 59/100 - Loss: -1.653   noise: 0.003\n",
      "Iter 60/100 - Loss: -1.680   noise: 0.003\n",
      "Iter 61/100 - Loss: -1.711   noise: 0.003\n",
      "Iter 62/100 - Loss: -1.747   noise: 0.002\n",
      "Iter 63/100 - Loss: -1.791   noise: 0.002\n",
      "Iter 64/100 - Loss: -1.847   noise: 0.002\n",
      "Iter 65/100 - Loss: -1.913   noise: 0.002\n",
      "Iter 66/100 - Loss: -1.983   noise: 0.002\n",
      "Iter 67/100 - Loss: -2.051   noise: 0.002\n",
      "Iter 68/100 - Loss: -2.111   noise: 0.002\n",
      "Iter 69/100 - Loss: -2.162   noise: 0.001\n",
      "Iter 70/100 - Loss: -2.206   noise: 0.001\n",
      "Iter 71/100 - Loss: -2.244   noise: 0.001\n",
      "Iter 72/100 - Loss: -2.277   noise: 0.001\n",
      "Iter 73/100 - Loss: -2.307   noise: 0.001\n",
      "Iter 74/100 - Loss: -2.337   noise: 0.001\n",
      "Iter 75/100 - Loss: -2.366   noise: 0.001\n",
      "Iter 76/100 - Loss: -2.397   noise: 0.001\n",
      "Iter 77/100 - Loss: -2.427   noise: 0.001\n",
      "Iter 78/100 - Loss: -2.458   noise: 0.001\n",
      "Iter 79/100 - Loss: -2.490   noise: 0.001\n",
      "Iter 80/100 - Loss: -2.522   noise: 0.001\n",
      "Iter 81/100 - Loss: -2.553   noise: 0.001\n",
      "Iter 82/100 - Loss: -2.585   noise: 0.001\n",
      "Iter 83/100 - Loss: -2.616   noise: 0.000\n",
      "Iter 84/100 - Loss: -2.646   noise: 0.000\n",
      "Iter 85/100 - Loss: -2.676   noise: 0.000\n",
      "Iter 86/100 - Loss: -2.704   noise: 0.000\n",
      "Iter 87/100 - Loss: -2.731   noise: 0.000\n",
      "Iter 88/100 - Loss: -2.756   noise: 0.000\n",
      "Iter 89/100 - Loss: -2.779   noise: 0.000\n",
      "Iter 90/100 - Loss: -2.801   noise: 0.000\n",
      "Iter 91/100 - Loss: -2.821   noise: 0.000\n",
      "Iter 92/100 - Loss: -2.839   noise: 0.000\n",
      "Iter 93/100 - Loss: -2.855   noise: 0.000\n",
      "Iter 94/100 - Loss: -2.869   noise: 0.000\n",
      "Iter 95/100 - Loss: -2.882   noise: 0.000\n",
      "Iter 96/100 - Loss: -2.894   noise: 0.000\n",
      "Iter 97/100 - Loss: -2.904   noise: 0.000\n",
      "Iter 98/100 - Loss: -2.913   noise: 0.000\n",
      "Iter 99/100 - Loss: -2.922   noise: 0.000\n",
      "Iter 100/100 - Loss: -2.930   noise: 0.000\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 100\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        # model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
